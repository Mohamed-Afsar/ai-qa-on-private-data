{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec3f68b1-c028-4146-960a-0285b98284c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0aee5e-1545-4fa4-b897-5500b74e8ac0",
   "metadata": {},
   "source": [
    "#### Function that loads a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5df5c97-a87b-4a21-90a6-424ac2f61058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    print(f'Loading {file}...')\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc377b8-9525-4ded-a77b-3b1218e403b7",
   "metadata": {},
   "source": [
    "#### Load a pdf file data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c0406bf-2da8-4008-979a-55d67b0f845b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Obtaining dependency information for pypdf from https://files.pythonhosted.org/packages/74/a9/5ccde1312650dd03e65350224fea85d9a430c182a01f056599cbb76f7390/pypdf-3.17.0-py3-none-any.whl.metadata\n",
      "  Using cached pypdf-3.17.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Using cached pypdf-3.17.0-py3-none-any.whl (277 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-3.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "632e90e4-a1af-4630-98bc-50f36f876667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./files/us_constitution.pdf...\n",
      "The\n",
      "United\n",
      "States\n",
      "Constitution\n",
      "W e\n",
      "the\n",
      "People\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "in\n",
      "Order\n",
      "to\n",
      "form\n",
      "a\n",
      "more\n",
      "perfect\n",
      "Union,\n",
      "establish\n",
      "Justice,\n",
      "insure\n",
      "domestic\n",
      "T ranquility ,\n",
      "provide\n",
      "for\n",
      "the\n",
      "common\n",
      "defence,\n",
      "promote\n",
      "the\n",
      "general\n",
      "W elfare,\n",
      "and\n",
      "secure\n",
      "the\n",
      "Blessings\n",
      "of\n",
      "Liberty\n",
      "to\n",
      "ourselves\n",
      "and\n",
      "our\n",
      "Posterity ,\n",
      "do\n",
      "ordain\n",
      "and\n",
      "establish\n",
      "this\n",
      "Constitution\n",
      "for\n",
      "the\n",
      "United\n",
      "States\n",
      "of\n",
      "America.\n",
      "The\n",
      "Constitutional\n",
      "Con v ention\n",
      "Article\n",
      "I\n",
      "Section\n",
      "1:\n",
      "Congress\n",
      "All\n",
      "legislative\n",
      "Powers\n",
      "herein\n",
      "granted\n",
      "shall\n",
      "be\n",
      "vested\n",
      "in\n",
      "a\n",
      "Congress\n",
      "of\n",
      "the\n",
      "United\n",
      "States,\n",
      "which\n",
      "shall\n",
      "consist\n",
      "of\n",
      "a\n",
      "Senate\n",
      "and\n",
      "House\n",
      "of\n",
      "Representatives.\n",
      "Section\n",
      "2:\n",
      "The\n",
      "House\n",
      "of\n",
      "Representatives\n"
     ]
    }
   ],
   "source": [
    "data = load_document('./files/us_constitution.pdf')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5535a34a-4fca-424f-97dd-26405215237d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './files/us_constitution.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e1faf2-8973-4809-b16e-e9d9c3724339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 41 pages in your pdf\n"
     ]
    }
   ],
   "source": [
    "print(f'You have {len(data)} pages in your pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6fac33-bd4d-4d1d-82c1-4d12fddbd11e",
   "metadata": {},
   "source": [
    "#### Improve load document function to also load a docx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1295b425-040f-48ed-81ae-af8644fed3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.8-py3-none-any.whl\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0687605-de24-4905-927c-81acd10dc8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "    print(f'Loading {file}...')\n",
    "    if extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print('Document format is not supported')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e998e626-5934-4527-a33d-807747277890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files/the_great_gatsby.docx...\n",
      "{'source': 'files/the_great_gatsby.docx'}\n"
     ]
    }
   ],
   "source": [
    "data = load_document('files/the_great_gatsby.docx')\n",
    "print(data[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fa25df3-0ad9-49bf-9e3e-cba9776fc2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c1ed2-32e1-456d-8176-5ab16c607d74",
   "metadata": {},
   "source": [
    "### Loading from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f012d7f-521d-4a5d-8a11-79c1d8c1a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc761658-c56c-43eb-8f38-07b612de32c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (from wikipedia) (4.12.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from wikipedia) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.5)\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1530aae8-9f4d-455c-9084-362e3c4f6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_from_wikipedia('GPT-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1a12423-a998-4088-98b0-5adceddc1346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model created by OpenAI, and the fourth in its series of GPT foundation models. It was initially released on March 14, 2023, and has been made publicly available via the paid chatbot product ChatGPT Plus, and via OpenAI's API.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.: 2 Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4 is also capable of taking images as input on ChatGPT. OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.\n",
      "\n",
      "\n",
      "== Background ==\n",
      " \n",
      "OpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\n",
      "Rumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\n",
      "\n",
      "\n",
      "== Capabilities ==\n",
      "OpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams.To gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.When instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model's prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.A 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing code and suggesting optimizations to improve performance. The article quoted a biophysicist who found that the time he required to port one of his programs from MATLAB to Python went down from days to \"an hour or so\". On a test of 89 security scenarios, GPT-4\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "162e4afc-7e6f-461e-ba4d-2a4d8538a965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 (acronyme de Generative Pre-trained Transformer 4) est un modèle de langage multimodalmultimodal, de type transformeur génératif pré-entraîné, développé par la société OpenAI et sorti le 14 mars 2023, il succède à GPT-3.\n",
      "\n",
      "\n",
      "== Caractéristiques ==\n",
      "OpenAI annonce, sur son blog, GPT-4 comme étant « plus fiable, créatif et capable de gérer des instructions beaucoup plus nuancées que GPT-3.5 ». L'organisation a produit deux versions de GPT-4 avec des fenêtres contextuelles de 8 192 et 32 768 jetons, une amélioration significative par rapport à GPT-3.5 et GPT-3, qui étaient limités à 4 096 et 2 048 jetons respectivement. Contrairement à son prédécesseur, GPT-4 peut prendre des images ainsi que du texte comme entrées.\n",
      "OpenAI adopte une approche fermée en ce qui concerne les détails techniques de GPT-4 ; le rapport technique s'est explicitement abstenu de spécifier la taille, l'architecture, le matériel ou la méthode de formation du modèle. Le rapport affirme que « le paysage concurrentiel et les implications sur la sécurité des modèles à grande échelle » sont des facteurs qui ont influencé cette décision. Le nombre précis de paramètres de GPT-4 reste inconnu, mais The Verge cite des rumeurs selon lesquelles GPT-4 augmenterait considérablement le nombre de paramètres de GPT-3 de 175 milliards à 100 000 milliards. Le PDG d'OpenAI, Sam Altman, qualifie ces rumeurs de « conneries complètes ». Un nombre plus fréquemment avancé est celui de 1000 milliards de paramètres,.\n",
      "Les représentants américains Don Beyer et Ted Lieu ont confirmé au New York Times qu'Altman s'était rendu au Congrès des États-Unis en janvier 2023 pour faire la démonstration du GPT-4 et de ses \"contrôles de sécurité\" améliorés par rapport aux autres modèles d'IA.\n",
      "\n",
      "\n",
      "=== Limites ===\n",
      "Aussi, même si cette version est améliorée par rapport à son prédécesseur, GPT-4 peut halluciner et présenter des biais.\n",
      "\n",
      "\n",
      "== Applications ==\n",
      "\n",
      "\n",
      "=== ChatGPT Plus ===\n",
      "GPT-4 est accessible via une application web appelée ChatGPT Plus, version payante de l'application gratuite ChatGPT.\n",
      "Des modules complémentaires sont développés par des tiers pour étendre les fonctionnalités de ChatGPT Plus et connecter le robot à leurs propres services. Open AI développe également ses propres modules dont Advanced data analysis (précédemment appelé Code interpreter). Ce dernier permet au robot de traiter des fichiers en exécutant lui-même du code Python, ce qui permet à l'utilisateur de réaliser des traitements de données et des graphiques en langage naturel. Le module améliore également les capacités de résolution de problèmes en mathématiques, physique et chimie.\n",
      "\n",
      "\n",
      "=== Bing ===\n",
      "En février 2023, Microsoft dote son moteur de recherche Bing d'un agent conversationnel reposant sur la technologie GPT-4. Cet agent répond aux questions de l'utilisateur en effectuant des recherches web. Contrairement à l'application d'Open AI, il cite ses sources, ce qui permet à l'utilisateur de les consulter lui-même.\n",
      "\n",
      "\n",
      "== Réception ==\n",
      "Pour le New York Times, GPT-4 est plus précis que GPT-3.5, il est capable de résumer et de commenter des images ainsi que des textes compliqués. Le journal affirme également qu'il a réussi un examen du barreau et plusieurs tests standardisés, mais qu'il montre cependant une tendance à halluciner les réponses.\n",
      "\n",
      "\n",
      "== Notes et références ==\n",
      "\n",
      "\n",
      "== Annexes ==\n",
      "\n",
      "\n",
      "=== Articles connexes ===\n",
      "BERT (modèle de langage)\n",
      "ChatGPT\n",
      "Intelligence artificielle\n",
      "Intelligence artificielle générale\n",
      "\n",
      "\n",
      "=== Liens externes ===\n",
      "\n",
      "(en) Site officiel\n",
      "\n",
      " Portail de l’informatique\n"
     ]
    }
   ],
   "source": [
    "data = load_from_wikipedia('GPT-4', 'fr')\n",
    "print(data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccf7cc1-d908-49d1-bcc9-15b145711b85",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d625ba8-0738-4797-97d6-51372cd52f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data, chunk_size=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
    "    chunks = splitter.split_documents(data)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "db98a89d-0d5c-4fa7-8fef-fde839b2233c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb658a64-2c84-41de-953c-51c47cd48087",
   "metadata": {},
   "source": [
    "#### Calculating embedding cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91dbb37b-eea7-4b50-b049-a1c94c8235a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in /opt/homebrew/lib/python3.11/site-packages (0.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/homebrew/lib/python3.11/site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c3065956-b85d-4299-b378-847fa443f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc = tiktoken.encoding_for_model('text-embedding-ada-002')\n",
    "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "    print(f'Total tokens: {total_tokens}')\n",
    "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.0004:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6923d16b-92b0-48cd-9b02-ecb95b17d2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 2079\n",
      "Embedding Cost in USD: 0.000832\n"
     ]
    }
   ],
   "source": [
    "print_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a276eeb-2c38-4515-95ed-ac6757fffb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
